{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 및 모듈 불러오기\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import sys\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\dhshs\\Documents\\ganomaly_global\")\n",
    "sys.path.append(r\"C:\\Users\\dhshs\\Documents\\ganomaly_global\\lib\")\n",
    "from lib.model import Ganomaly  # lib/model.py에서 제공되는 GANomaly 모델 사용\n",
    "from lib.evaluate import evaluate, roc  # evaluate.py에서 제공되는 평가 함수 사용\n",
    "from lib.visualizer import Visualizer  # visualizer.py에서 제공되는 시각화 클래스 사용\n",
    "from options import Options  # 옵션 파일\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, directory, label, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.label = label  # 레이블 정보 추가\n",
    "        self.images = [img for img in os.listdir(directory) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.directory, self.images[idx])\n",
    "        image = Image.open(img_path).convert(\"L\")  # 흑백 이미지로 변환\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(self.label, dtype=torch.long)  # 레이블 포함하여 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로 설정\n",
    "normal_train_dir = r\"C:\\Users\\dhshs\\Documents\\ganomaly_global\\dataset\\ELPV\\mono\\train\\0.normal\"\n",
    "abnormal_test_dir = r\"C:\\Users\\dhshs\\Documents\\ganomaly_global\\dataset\\ELPV\\mono\\test\\1.abnormal\"\n",
    "normal_test_dir = r\"C:\\Users\\dhshs\\Documents\\ganomaly_global\\dataset\\ELPV\\mono\\test\\0.normal\"\n",
    "\n",
    "# 데이터 전처리 및 데이터 로더 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # 이미지 크기 조정\n",
    "    transforms.ToTensor(),  # 텐서로 변환\n",
    "])\n",
    "\n",
    "# 학습 및 테스트 데이터셋 생성\n",
    "train_dataset = CustomDataset(normal_train_dir, label=0, transform=transform)  # 학습 데이터셋 로드 (normal만)\n",
    "normal_test_dataset = CustomDataset(normal_test_dir, label=0, transform=transform)  # 테스트 데이터셋 로드 (normal)\n",
    "abnormal_test_dataset = CustomDataset(abnormal_test_dir, label=1, transform=transform)  # 테스트 데이터셋 로드 (abnormal)\n",
    "\n",
    "# 두 개의 테스트 데이터셋을 합침\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "test_dataset = ConcatDataset([normal_test_dataset, abnormal_test_dataset])\n",
    "\n",
    "# 데이터 로더 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# LaTeX 사용 비활성화\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "\n",
    "# 데이터 로더에서 샘플 하나 가져오기\n",
    "sample_image = next(iter(train_loader))\n",
    "\n",
    "# 첫 번째 이미지 선택 (배치 크기만큼 가져오므로 첫 번째만 선택)\n",
    "sample_image = sample_image[0][0].squeeze(0)  # 배치 차원과 채널 차원을 제거\n",
    "\n",
    "# 이미지 시각화\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.imshow(sample_image, cmap='gray')\n",
    "plt.title(\"Sample Image\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook의 매개변수를 무시하도록 설정\n",
    "sys.argv = ['']\n",
    "\n",
    "# 옵션 설정\n",
    "opt = Options().parse()\n",
    "opt.isize = 32  # 데이터의 이미지 크기\n",
    "opt.batchsize = 32  # 배치 크기 설정\n",
    "opt.save_test_images = True  # 테스트 이미지 저장 설정\n",
    "opt.display = True  # 실시간 시각화 설정\n",
    "opt.use_context_pred = True # 마스킹 설정\n",
    "\n",
    "opt.outf = './output(ELPV with mask)'\n",
    "opt.nz = 500    # 잠재 벡터 차원 수 \n",
    "opt.nc = 1       # 채널 수\n",
    "opt.niter = 50 # 에폭 수\n",
    "opt.context_mask_size = 64 # 마스킹 블록 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {'train': train_loader, 'test': test_loader}  # 데이터 로더 설정\n",
    "\n",
    "# GANomaly 모델 생성\n",
    "model = Ganomaly(opt, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 추가 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook의 매개변수를 무시하도록 설정\n",
    "sys.argv = ['']\n",
    "\n",
    "# 옵션 설정\n",
    "opt = Options().parse()\n",
    "opt.isize = 256  # 데이터의 이미지 크기\n",
    "opt.nc = 1       # 흑백 이미지이므로 채널 수를 1로 설정\n",
    "opt.batchsize = 32  # 배치 크기 설정\n",
    "opt.save_test_images = True  # 테스트 이미지 저장 설정\n",
    "opt.display = True  # 실시간 시각화 설정\n",
    "opt.niter = 50\n",
    "opt.resume = {\n",
    "    'netg_path': r\"C:\\Users\\dhshs\\Documents\\ganomaly_global\\project\\output(use masking 가중치 0.5)\\ganomaly\\ELPV\\train\\weights\\epoch50_netG.pth\",\n",
    "    'netd_path': r\"C:\\Users\\dhshs\\Documents\\ganomaly_global\\project\\output(use masking 가중치 0.5)\\ganomaly\\ELPV\\train\\weights\\epoch50_netD.pth\"\n",
    "}\n",
    "opt.use_context_pred = True # 마스킹 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {'train': train_loader, 'test': test_loader}  # 데이터 로더 설정\n",
    "\n",
    "# GANomaly 모델 생성\n",
    "model = Ganomaly(opt, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_visualize(model, dataloader, device, threshold=None):\n",
    "    \"\"\"\n",
    "    Evaluate the model and visualize the results, including test(normal) and test(anomaly) distributions.\n",
    "    This function integrates the calculation of novelty scores.\n",
    "    \"\"\"\n",
    "    model.netg.eval()  # Set the model to evaluation mode\n",
    "    novelty_scores = []\n",
    "    labels = []\n",
    "\n",
    "    # Calculate novelty scores\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, targets = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Forward pass through NetG\n",
    "            _, latent_i, latent_o = model.netg(inputs)\n",
    "\n",
    "            # Calculate novelty score\n",
    "            scores = torch.mean((latent_i - latent_o) ** 2, dim=1).cpu().numpy().flatten()\n",
    "            novelty_scores.extend(scores)\n",
    "            labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    novelty_scores = np.array(novelty_scores)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Find threshold if not provided\n",
    "    if threshold is None:\n",
    "        fpr, tpr, thresholds = roc_curve(labels, novelty_scores)\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        threshold = thresholds[optimal_idx]\n",
    "\n",
    "    # Generate predictions\n",
    "    preds = (novelty_scores > threshold).astype(int)\n",
    "\n",
    "    # Separate test scores into normal and anomaly based on true labels\n",
    "    normal_scores = novelty_scores[labels == 0]\n",
    "    anomaly_scores = novelty_scores[labels == 1]\n",
    "\n",
    "    # Plot Novelty Scores Distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(normal_scores, color=\"green\", label=\"Test (Normal)\", kde=True, bins=30, stat=\"density\", alpha=0.5)\n",
    "    sns.histplot(anomaly_scores, color=\"red\", label=\"Test (Anomalous)\", kde=True, bins=30, stat=\"density\", alpha=0.5)\n",
    "    plt.axvline(x=threshold, color=\"black\", linestyle=\"--\", label=\"Threshold\")\n",
    "    plt.title(\"Novelty Scores Distribution\")\n",
    "    plt.xlabel(\"Novelty Score\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.xlim(0, 1)  # X-axis range limited to 0-1\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Print Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, preds))\n",
    "\n",
    "    return threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and visualize\n",
    "optimal_threshold = evaluate_and_visualize(model, test_loader, device)\n",
    "print(f\"Optimal Threshold: {optimal_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_test_dataset(model, test_loader, threshold, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    설정된 임계값을 사용하여 test dataset에 대해 이상치를 판별하고 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        model: GANomaly 모델 인스턴스\n",
    "        test_loader: test sample을 포함한 DataLoader\n",
    "        threshold (float): 이상치 판별을 위한 임계값\n",
    "        device (str): 연산에 사용할 장치 (기본값: \"cuda\")\n",
    "    \"\"\"\n",
    "    model.netg.eval()\n",
    "    novelty_scores = []\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            # 데이터 로드 및 device로 이동\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            if inputs.dim() == 3:\n",
    "                inputs = inputs.unsqueeze(0)\n",
    "            \n",
    "            # latent 벡터 생성 및 novelty score 계산\n",
    "            _, latent_i, latent_o = model.netg(inputs)\n",
    "            scores = torch.mean((latent_i - latent_o) ** 2, dim=1)\n",
    "            novelty_scores.extend(scores.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # 임계값을 사용해 이상 여부를 예측 (0: 정상, 1: 이상)\n",
    "            pred = (scores >= threshold).int()\n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "\n",
    "    # numpy 배열로 변환 및 차원 축소\n",
    "    true_labels = np.array(true_labels).astype(int).flatten()\n",
    "    predictions = np.array(predictions).astype(int).flatten()\n",
    "    novelty_scores = np.array(novelty_scores).flatten()\n",
    "\n",
    "    # Confusion matrix 생성 및 시각화\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xticks([0, 1], [\"Normal\", \"Anomaly\"])\n",
    "    plt.yticks([0, 1], [\"Normal\", \"Anomaly\"])\n",
    "\n",
    "    # 각 셀에 샘플 개수를 텍스트로 추가\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 예시: test_loader를 입력으로 전달하고 threshold 적용\n",
    "evaluate_test_dataset(model, test_loader, optimal_threshold, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook의 매개변수를 무시하도록 설정\n",
    "sys.argv = ['']\n",
    "\n",
    "# 옵션 설정\n",
    "opt = Options().parse()\n",
    "opt.isize = 256  # 데이터의 이미지 크기\n",
    "opt.nc = 1       # 흑백 이미지이므로 채널 수를 1로 설정\n",
    "opt.batchsize = 32  # 배치 크기 설정\n",
    "opt.save_test_images = True  # 테스트 이미지 저장 설정\n",
    "opt.display = True  # 실시간 시각화 설정\n",
    "opt.niter = 50\n",
    "opt.resume = {\n",
    "    'netg_path': r\"C:\\Users\\dhshs\\Documents\\ganomaly_global\\project\\output(use masking 가중치 0.5)\\ganomaly\\ELPV\\train\\weights\\epoch50_netG.pth\",\n",
    "    'netd_path': r\"C:\\Users\\dhshs\\Documents\\ganomaly_global\\project\\output(use masking 가중치 0.5)\\ganomaly\\ELPV\\train\\weights\\epoch50_netD.pth\"\n",
    "}\n",
    "opt.use_context_pred = True # 마스킹 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {'train': train_loader, 'test': test_loader}  # 데이터 로더 설정\n",
    "\n",
    "# GANomaly 모델 생성\n",
    "model = Ganomaly(opt, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "# CIFAR-10 클래스 레이블\n",
    "cifar10_labels = {\n",
    "    0: \"airplane\", 1: \"automobile\", 2: \"bird\", 3: \"cat\", 4: \"deer\",\n",
    "    5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\"\n",
    "}\n",
    "\n",
    "# CIFAR-100 클래스 레이블\n",
    "cifar100_labels = {\n",
    "    0: \"apple\", 1: \"aquarium_fish\", 2: \"baby\", 3: \"bear\", 4: \"beaver\",\n",
    "    5: \"bed\", 6: \"bee\", 7: \"beetle\", 8: \"bicycle\", 9: \"bottle\",\n",
    "    10: \"bowl\", 11: \"boy\", 12: \"bridge\", 13: \"bus\", 14: \"butterfly\",\n",
    "    15: \"camel\", 16: \"can\", 17: \"castle\", 18: \"caterpillar\", 19: \"cattle\",\n",
    "    20: \"chair\", 21: \"chimpanzee\", 22: \"clock\", 23: \"cloud\", 24: \"cockroach\",\n",
    "    25: \"couch\", 26: \"crab\", 27: \"crocodile\", 28: \"cup\", 29: \"dinosaur\",\n",
    "    30: \"dolphin\", 31: \"elephant\", 32: \"flatfish\", 33: \"forest\", 34: \"fox\",\n",
    "    35: \"girl\", 36: \"hamster\", 37: \"house\", 38: \"kangaroo\", 39: \"keyboard\",\n",
    "    40: \"lamp\", 41: \"lawn_mower\", 42: \"leopard\", 43: \"lion\", 44: \"lizard\",\n",
    "    45: \"lobster\", 46: \"man\", 47: \"maple_tree\", 48: \"motorcycle\", 49: \"mountain\",\n",
    "    50: \"mouse\", 51: \"mushroom\", 52: \"oak_tree\", 53: \"orange\", 54: \"orchid\",\n",
    "    55: \"otter\", 56: \"palm_tree\", 57: \"pear\", 58: \"pickup_truck\", 59: \"pine_tree\",\n",
    "    60: \"plain\", 61: \"plate\", 62: \"poppy\", 63: \"porcupine\", 64: \"possum\",\n",
    "    65: \"rabbit\", 66: \"raccoon\", 67: \"ray\", 68: \"road\", 69: \"rocket\",\n",
    "    70: \"rose\", 71: \"sea\", 72: \"seal\", 73: \"shark\", 74: \"shrew\",\n",
    "    75: \"skunk\", 76: \"skyscraper\", 77: \"snail\", 78: \"snake\", 79: \"spider\",\n",
    "    80: \"squirrel\", 81: \"streetcar\", 82: \"sunflower\", 83: \"sweet_pepper\", 84: \"table\",\n",
    "    85: \"tank\", 86: \"telephone\", 87: \"television\", 88: \"tiger\", 89: \"tractor\",\n",
    "    90: \"train\", 91: \"trout\", 92: \"tulip\", 93: \"turtle\", 94: \"wardrobe\",\n",
    "    95: \"whale\", 96: \"willow_tree\", 97: \"wolf\", 98: \"woman\", 99: \"worm\"\n",
    "}\n",
    "\n",
    "# 다운로드 경로 설정\n",
    "base_path = './cifar_filtered/'\n",
    "\n",
    "# CIFAR-10 데이터 로드\n",
    "cifar10 = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "cifar100 = datasets.CIFAR100(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# 원하는 클래스 필터링\n",
    "target_classes_cifar10 = ['cat']\n",
    "target_classes_cifar100 = ['lion']\n",
    "\n",
    "target_indices_cifar10 = [key for key, value in cifar10_labels.items() if value in target_classes_cifar10]\n",
    "target_indices_cifar100 = [key for key, value in cifar100_labels.items() if value in target_classes_cifar100]\n",
    "\n",
    "# cat과 lion 각각 저장할 폴더 생성\n",
    "cat_path = os.path.join(base_path, 'cat')\n",
    "lion_path = os.path.join(base_path, 'lion')\n",
    "\n",
    "os.makedirs(cat_path, exist_ok=True)\n",
    "os.makedirs(lion_path, exist_ok=True)\n",
    "\n",
    "# CIFAR-10에서 cat 저장\n",
    "for idx, (image, label) in enumerate(cifar10):\n",
    "    if label in target_indices_cifar10:\n",
    "        save_path = os.path.join(cat_path, f'CIFAR10_cat_{idx}.png')\n",
    "        transforms.ToPILImage()(image).save(save_path)\n",
    "\n",
    "# CIFAR-100에서 lion 저장\n",
    "for idx, (image, label) in enumerate(cifar100):\n",
    "    if label in target_indices_cifar100:\n",
    "        save_path = os.path.join(lion_path, f'CIFAR100_lion_{idx}.png')\n",
    "        transforms.ToPILImage()(image).save(save_path)\n",
    "\n",
    "print(f\"Filtered images saved in {base_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from random import shuffle\n",
    "\n",
    "# 기존 cat 폴더 경로\n",
    "cat_path = './cifar_filtered/cat'\n",
    "\n",
    "# 새로 나눌 폴더 경로\n",
    "split_3000_path = './cifar_filtered/cat_3000'\n",
    "split_2000_path = './cifar_filtered/cat_2000'\n",
    "\n",
    "# 새 폴더 생성\n",
    "os.makedirs(split_3000_path, exist_ok=True)\n",
    "os.makedirs(split_2000_path, exist_ok=True)\n",
    "\n",
    "# cat 폴더에서 이미지 파일 목록 불러오기\n",
    "cat_images = [f for f in os.listdir(cat_path) if os.path.isfile(os.path.join(cat_path, f))]\n",
    "\n",
    "# 이미지 섞기 (랜덤화)\n",
    "shuffle(cat_images)\n",
    "\n",
    "# 3,000장과 2,000장으로 분리\n",
    "split_3000 = cat_images[:3000]\n",
    "split_2000 = cat_images[3000:5000]\n",
    "\n",
    "# 이미지 이동\n",
    "for img in split_3000:\n",
    "    shutil.move(os.path.join(cat_path, img), os.path.join(split_3000_path, img))\n",
    "\n",
    "for img in split_2000:\n",
    "    shutil.move(os.path.join(cat_path, img), os.path.join(split_2000_path, img))\n",
    "\n",
    "print(f\"Split complete: 3000 images in {split_3000_path}, 2000 images in {split_2000_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
